{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import librosa\n",
    "\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tp = pd.read_csv('/kaggle/input/rfcx-species-audio-detection/train_tp.csv')\n",
    "data_fp = pd.read_csv('/kaggle/input/rfcx-species-audio-detection/train_fp.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try to load a single `flac` file and generate Mel Spectrogram From It\n",
    "* load from flac file\n",
    "    * targets are all the same bitrate, convenient\n",
    "* get the slice of audio annotated from in `data`\n",
    "    * sampling rate is 48k - i.e. each second of audio corresponds to 48k numbers in the array\n",
    "    * to get the t second point, start at the (48k * t)th number\n",
    "* generate the mel spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLING_RATE = 48000\n",
    "F_MAX = max(data_tp['f_max']) * 0.9\n",
    "F_MIN = min(data_tp['f_min']) * 1.1\n",
    "\n",
    "window_size = 512\n",
    "between_window = 256\n",
    "power = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_name = '/kaggle/input/rfcx-species-audio-detection/train/' + data_tp['recording_id'][0] + '.flac'\n",
    "wav, sr = librosa.load(test_file_name)\n",
    "print(\"sr =\", sr)\n",
    "\n",
    "t_begin, t_end = data_tp['t_min'][0], data_tp['t_max'][0]\n",
    "clipped_wav = wav[int(t_begin*sr) : int(t_end*sr)]\n",
    "print(\"t_begin = {}, t_end = {}\".format(t_begin, t_end))\n",
    "\n",
    "ms = librosa.feature.melspectrogram(\n",
    "    clipped_wav, sr = sr, n_fft = window_size, hop_length = between_window, power = power\n",
    ")\n",
    "\n",
    "print(\"ms.shape = {}\".format(ms.shape))\n",
    "# ms.shape = (n_mels, len(clipped_wav)/between_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.display(display.Audio(clipped_wav, rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "ms_norm = ms/ms.max()\n",
    "plt.imshow(ms_norm.T)\n",
    "plt.title(\"Normalized Spectogram\");\n",
    "\n",
    "plt.figure(1)\n",
    "ms_norm_per_window = ms/ms.max(axis=1)[..., None]\n",
    "plt.imshow(ms_norm_per_window.T)\n",
    "plt.title(\"Normalized Spectrum for Each Time Window\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How long are the clips?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data_tp['t_max'] - data_tp['t_min'], bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Each Image and Save as Mel Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLING_RATE = 48000\n",
    "F_MAX = max(data_tp['f_max']) * 0.8\n",
    "F_MIN = min(data_tp['f_min']) * 1.2\n",
    "HEIGHT, WIDTH = (224, 512) # I consistently see people reshaping to this size in notebooks; no idea why\n",
    "clip_length = 3.5 # always try to get a clip of this long in seconds, centered around the given interval\n",
    "\n",
    "window_size = 1024\n",
    "between_window = 512\n",
    "power = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage.interpolation import zoom\n",
    "\n",
    "def to_ms(file_in, t_begin, t_end, plot=False):\n",
    "    wav, sr = librosa.load(file_in)\n",
    "    midpoint = int((t_begin + t_end)*sr/2)\n",
    "    \n",
    "    clipped_wav = wav[max(int(midpoint - clip_length*sr/2),0):int(midpoint+clip_length*sr/2)]\n",
    "\n",
    "    ms = librosa.feature.melspectrogram(\n",
    "        clipped_wav, sr = sr,\n",
    "        n_fft = window_size, hop_length = between_window,\n",
    "        power = power, fmin = F_MIN, fmax = F_MAX\n",
    "    )\n",
    "    \n",
    "    # normalize ms\n",
    "    ms = ((ms - ms.min())/(ms.max() - ms.min())).T\n",
    "    \n",
    "    # resize ms\n",
    "    # scipy uses spline interpolation as opposed to skimage.transform.resize that takes a local average\n",
    "    h,w = ms.shape\n",
    "    ms = zoom(ms, (HEIGHT/h, WIDTH/w))\n",
    "    assert ms.shape == (HEIGHT, WIDTH)\n",
    "\n",
    "    if plot:\n",
    "        plt.imshow(ms)\n",
    "        \n",
    "    return ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 32\n",
    "file_name = '/kaggle/input/rfcx-species-audio-detection/train/' + data_tp['recording_id'][i] + '.flac'\n",
    "t_begin, t_end = data_tp['t_min'][i], data_tp['t_max'][i]\n",
    "\n",
    "mss = to_ms(file_name, t_begin, t_end, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs = []\n",
    "OUTPUT = '/kaggle/working/spectrograms.pkl'\n",
    "\n",
    "for i, row in data_tp.iterrows():\n",
    "    ms = to_ms(\n",
    "        '/kaggle/input/rfcx-species-audio-detection/train/' + row['recording_id'] + '.flac',\n",
    "        row['t_min'],\n",
    "        row['t_max']\n",
    "    )\n",
    "    \n",
    "    specs.append((ms, row['species_id'], i))\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print('{}/{} ({}%) processed'.format(i, len(data_tp), i/len(data_tp)))\n",
    "        \n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        with open(OUTPUT, 'wb') as f:\n",
    "            pickle.dump(specs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 500\n",
    "\n",
    "class CustomTensorDataset(Dataset):\n",
    "    def __init__(self, tensors, transform=None):\n",
    "        assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)\n",
    "        self.tensors = tensors\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.tensors[0][index]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        y = self.tensors[1][index]\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tensors[0].size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('spectrograms.pkl', 'rb') as f:\n",
    "    spectrograms = pickle.load(f)\n",
    "    \n",
    "len(spectrograms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spectrograms is a list of tuples:\n",
    "#    (spectrogram [np.Array], class (int), true positive dataframe index (int))\n",
    "X_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "X = torch.tensor(np.array([tup[0] for tup in spectrograms]))\n",
    "Y = torch.tensor(np.array([tup[1] for tup in spectrograms]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = CustomTensorDataset(tensors = (X,Y), transform=X_transform)\n",
    "dl = DataLoader(d, batch_size = BATCH_SIZE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
